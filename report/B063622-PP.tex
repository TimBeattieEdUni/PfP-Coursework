%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MSc HPC
% Performance Programming
% Coursework
% Exam no. B063622
%
% Report.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages.
%
\documentclass[11pt, oneside]{article}   % use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                    % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}            % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                    % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                         % TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{listings}


% load .eps files created by GnuPlot for epstopdf to convert to .pdf
\DeclareGraphicsExtensions{.eps}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Components.
\title{Performance Programming \\ Coursework}
\author{B063622}
\date{\today}


%
% The following defines an environment for including source with syntax hilighting.
% (Copied from stackoverflow.com/questions/3175105/how-to-insert-code-%into-a-latex-doc) 
% Could be useful if we want to include source code in the report. 
%
% To use a different language, overwrite the language paramter in the code. I.e. write: 
%	\lstset{language=bash}
% before the beginning of the listing. 

% Then enter your code: 
%	\begin{lstlisting}
%		source code here ...
%	\end{lstlisting}
%

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
language=c++,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=false,
breakatwhitespace=true,   
tabsize=3
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The document itself.
%
\begin{document}

\pagenumbering{gobble}   % No page number on title page.
\maketitle

\newpage

\pagenumbering{roman}   % Preamble pages numbered in Roman numerals.
\tableofcontents

\newpage

\pagenumbering{arabic}  % Body of report numbered in Arabic.

\section{Introduction}
The following report describes the process of optimising a program which runs a molecular dynamics calculation.
The calculation models a number of particles which are subject to two forces: an inverse square attraction and a viscosity in the system.

A series of  modifications were made to the code in an attempt to optimise its performance, including several which degraded performance and which were discarded from the final code.
The modifications these are described, and an analysis of the effect of each modification on the code's performance is presented.

The code was provided in both C and Fortran; the C version was chosen for this exercise.

\section{First Steps}

\subsection{Examining the Code}
\label{subsec:ExaminingTheCode}

The first step was to study the code and its build process in order to gain an understanding of how the code worked and to identify potential modifications which might improve performance.
Several such possibilities were initially identified in the code and its make file:

\begin{itemize}
	\item Debug symbols enabled during compilation.
	\item Lack of compiler optimisation.
	\item Nested for loops with slowest-changing indices innermost.
	\item Loops which could be combined.
	\item Repeated calculations within loops.
	\item Loop-invariant values recalculated within loops.
	\item Branches within loops.
	\item Arrays sized as powers of 2.
	\item Related data in separate structures in memory.
	\item Multidimensional array indexing which could be converted to incrementing pointers.
\end{itemize}


\subsection{Recording Initial Performance}
An initial timing run was performed with the code and makefile unmodified.
The only change made to the provided files was to modify the Sun Grid Engine batch script \textbf{bench\_c.sge} to allocate 64 cores for each run as shown in Figure~\ref{figure:batchsge}.
This was done in order to isolate timing runs from the effects of any other programs running on the same node on MORAR.
This setting was applied to all timing runs performed.

\begin{figure}
	\begin{lstlisting}
		#$ -l h\_rt=:20:
		#$ -pe mpi 64
		#$ -cwd
	\end{lstlisting}
	\caption{Modification to bench\_c.sge to allocate 64 cores for timing runs.}
	\label{figure:batchsge}
\end{figure}

The program performed five iterations of 100 steps with each step including the running count of collisions and with each set of 100 steps indicating the time taken for that set.
Times for the initial run are shown in Table~\ref{table:InitialRunTimes}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{||l|c|l||}
			\hline
				{\bf Steps} & {\bf Time}\\
			\hline
				100  &  193.724736\\
				200  &  193.554321\\
				300  &  193.863500\\
				400  &  193.568124\\
				500  &  193.937730\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Initial run times per 100 steps.}
	\label{table:InitialRunTimes}
\end{table}

The initial timing was very consistent, resulting in an average of 1.937 seconds per step, with times for each set of 100 steps varying by less than one percent.
Based on these timings, it was decided that only the first four figures were significant as the fourth digit is the first to show (small) variation between measurements.
For this reason, two runs of 100 steps each were considered sufficient for timing the first few modifications to the code.
It was anticipated that times might become less consistent when the program became more efficient as interruptions for I/O would affect the program more if it were keeping the CPU busy.
However, run times remained consistent to within approximately three percent throughout the work done, so the number of timing runs for each modification was not increased.

\subsection{Testing for Correctness}

The program printed the number of the current step and the running count of collisions to the standard output along with the time taken for each set of 100 steps.
The program also wrote the positions and velocities of each particle in the system to an output file after each set.
In order to consider the program to still be correct after a modification, both the particle data and the collision count had to be sufficiently accurate.  

Perfect accuracy was not required as the order of floating point arithmetic in the code might be altered.
A test tool was provided which compared the values in two output files and reported the maximum difference between corresponding values as well as any differences which were greater than a chosen minimum and which were therefore unacceptable.  

The tool had a bug: it failed to handle NaN values correctly.
The code which performed the comparison between values reported a difference of 0.0 between any floating point number and NaN.
A check for NaN values was added and the code was made to indicate that the difference between any valid value and NaN, or between two NaNs, was the largest available double-precision floating point value.
The modified code is shown in Figure~\ref{figure:NanTest}.

\begin{figure}
	\begin{lstlisting}
		double error(double v1, double v2)
		{
			//  check for invalid values and return a 
			//  value greater than any reasonable maximum delta
			if (isnan(v1) || isnan(v2))
			{
				return DBL_MAX;
			}

			double diff = fabs(v1 - v2);
			double sum = fabs(v1 + v2);
			if (sum != 0.0)
			{
				// normalise
				diff = diff / sum;
			}
			return diff;
		}
	\end{lstlisting}
	\caption{Checking for NaN values in diff tool.}
	\label{figure:NanTest}
\end{figure}

\subsection{Input Data for Testing}
In order to produce output which could be tested for accuracy by comparing it with output from previous runs, the set of particle starting positions and velocities provided as input to the program had to be the same for each test run.
Additionally the program had to be run until collisions started to occur in order to verify that the collision count was correct.
A run of 100 steps was considered sufficient for both timing and accuracy as molecular dynamics calculations are generally very sensitive to small perturbations after many iterations.  

To ensure that collision counts were correct, the input used was a set of particle positions and velocities produced by the original program at a number of steps after collisions had begun occurring.
It was found that collisions began at 162 steps, so the positions and velocities at 200 steps, written to file output.dat200 by the first run of the program, were used as test run inputs.
The terst run's output file output.dat100 was then compared with the file output.dat100 produced by a run of the original unmodified code with the same input.
The program's output to stdout was compared manually to verify that the number of collisions had not changed.

A small difference in the number of collisions would have been acceptable in the same way as small differences in particle positions and velocities.
However, the number of collisions at 100 steps when starting with the selected input data was always 34107; this did not change throughout the exercise.

\subsection{Work Process}
Each modification to the program was performed in the following steps:

\begin{enumerate}
	\item The code or makefile was modified and the application built.
	\item The program was run twice on MORAR with output.dat200 specified as its input file.
	\item The test program was used to verify the accuracy of the ouput.
	\item The number of collisions after 100 steps was checked for any difference with the original value.
	\item The time for 100 steps was checked for any performance improvement.
	\item If performance was significantly degraded by the modification, it was reverted. 
\end{enumerate}

Modifications to the code were kept small and done one at a time in order to isolate the effects of any change.
Exceptions to this occurred when one modification enabled another - for example where a function was called within a loop and inlining the function enabled the loop to be fused with other loops.

The accuracy of the final positions and velocities was verified using the test program.
The run time for 100 steps and the accuracy of the collision count were checked manually as this did not consume significant time.

\subsection{Profiling}

A profiling run was performed in order to gather information about the program's use of CPU time.
The profiling build was created by cleaning the previous build and passing the necessary profiling flags to the compiler on the command line as shown in Figure~\ref{figure:ProfMake}.

\begin{figure}
	\begin{lstlisting}
		make clean
		make CFLAGS="-g -pg"
	\end{lstlisting}
	\caption{Buildng with profiling enabled.}
	\label{figure:ProfMake}
\end{figure}

Output from the profiler is shown in Figure~\ref{figure:ProfOutput}.
\begin{figure}
	\begin{lstlisting}
		Each sample counts as 0.01 seconds.
		%   cumulative   self              self     total           
		time   seconds   seconds    calls   s/call   s/call  name    
		32.64    118.78   118.78                             _mcount2
		30.92    231.31   112.53 5033164800     0.00     0.00  force
		28.79    336.08   104.78        1   104.78   233.59  evolve
		4.46    352.31    16.23      600     0.03     0.03  add_norm
		2.48    361.33     9.02                             __forceEND
		0.52    363.23     1.89                             _mp_preinit
		0.17    363.83     0.60                             __rouinit
		0.01    363.86     0.03      300     0.00     0.00  visc_force
		0.01    363.88     0.02      300     0.00     0.00  wind_force
		0.01    363.90     0.02                             __rouexit
		0.00    363.90     0.00        2     0.00     0.00  second
	\end{lstlisting}
	\caption{Output from the profiler.}
	\label{figure:ProfOutput}
\end{figure}

It can be seen that most of the program's time is spent in functions \textbf{force()} and \textbf{evolve()}.
This result is unsurprising as function \textbf{evolve()} contains all of the program's calculation loops, and \textbf{force()} is called repeatedly in the innermost of a set of three nested for loops which together iterate over the entire range of four of the program's large arrays.

As function \textbf{evolve()} contained several for loops, the option of placing each of these in a separate function in order to use the profiler to determine which were taking the most time was considered.
However, there were already several clear possibilities for modifications (see Section~\ref{subsec:ExaminingTheCode}) which were expected to improve performance significantly, so this was reserved as an option for later if needed.

\section{Modifications to the Code}

\subsection{Removing Debug Symbols}
A quick and obvious modification to the program's makefile was to remove the \textbf{-g} flag from the CFLAGS variable as this adds debug symbols to the executable, increasing its size and degrading its performance.
Two timing runs were performed with the flag removed, and the resulting times for 100 steps were 107.066320 and 107.309783 seconds.
Taking the average of these times and comparing with the original value of 1.937 seconds per step, the reduction in run time was approximately 45\%.
This was unsurprising as enabling debug information when compiling typically reduces performance significantly.

\subsection{Compiler Optimisation Flags}
A second quick modification was to apply compiler optimisation flags in the program's makefile.
Flags \textbf{-O1} to \textbf{-O4} were tried as well as \textbf{-fast -Mipa=fast,inline}  as recommended by th PGI Compiler User Guide. \cite{ref:PgiCC}
Results are shown in Table~\ref{table:OptFlags}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{||l|c|l||}
			\hline
			{\bf Flags} & {\bf Time 1} & {\bf Time 2}\\
			\hline
				O1  &  125.075299  &  (n/a)\\
				O2  &  127.319100  &  (n/a)\\ 
				O3  &  127.375565  &  127.023777\\
				O4  &  125.678674  &  125.906789\\
				-fast -Mipa=fast,inline  &  95.240342  &  95.159813\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Run times with various compiler optimisations enabled.}
	\label{table:OptFlags}
\end{table}

A single run was done for each of \textbf{-O1} and \textbf{-O2} as these were not expected to significantly improve the program's 'run time.  
In fact both were found to increase run time.
Two runs were done for each of \textbf{-O3} and \textbf{-O4}, and these also increased run time.
Two runs were done with \textbf{-fast âMipa=fast,inline} and this was found to reduce run time by 11\% from the previous run time, representing a reduction of 51\% in run time from the original build of the program.

The compiler flags were therefore left as \textbf{-fast -Mipa=fast,inline} for subsequent modifications.

\subsection{Loop Swap in Calculation of Pairwise Separation}
The next modification was a loop swap in the program's calculation of distances between particles.
In the original code shown in Figure~\ref{figure:PairSepOrig} this was done with a set of three nested \textbf{for} loops with the slowest-moving index {\em l} in the innermost loop.

\begin{figure}
	\begin{lstlisting}
		/* calculate pairwise separation of particles */
		k = 0;
		for (i = 0; i < Nbody; i++)
		{
			for (j = i + 1; j < Nbody; j++)
			{
				for (l = 0; l < Ndim; l++)
				{
					delta_pos[l][k] = pos[l][i] - pos[l][j];
				}
				k = k + 1;
			}
		}
	\end{lstlisting}
	\caption{Original code with slowest index in innermost loop.}
	\label{figure:PairSepOrig}
\end{figure}

The innermost loop was moved to the outermost position and the initialisation and incrementing of variable {\em k} were moved to new positions.
The modified code is shown in Figure~\ref{figure:PairSepMod}.

\begin{figure}
	\begin{lstlisting}
		/* calculate pairwise separation of particles */
		for (l = 0; l < Ndim; l++)
		{
			k = 0;
			for (i = 0; i < Nbody; i++)
			{
				for (j = i + 1; j < Nbody; j++)
				{
					delta_pos[l][k] = pos[l][i] - pos[l][j];
					k = k + 1;
				}
			}
		}
	\end{lstlisting}
	\caption{Modified code with slowest index in outermost loop.}
	\label{figure:PairSepMod}
\end{figure}

Two timing runs of 100 steps produced times of 65.047681 and 65.043864 seconds, representing an average time per step of 0.650 seconds.
This represents a reduction in run time of 32\% over the previous code and a reduction of 66\% from the original program.

\subsection{Removal of Repeated Calculations}
The next modification was to remove two repeated calculations of the product of the gravitational constant and the masses of two particles in the calculation of the forces between pairs of particles.
The original code shown in Figure~\ref{figure:GmmOrig} calculates \textbf{G * mass[i] * mass[j]} twice unnecessarily.

This was taking place inside three nested for loops with the slowest-moving index in the innermost loop, and it was recognised that it would be beneficial to swap these loops in order to improve performance.
However, the original code was complicated enough that doing this took several attempts, and in order to make at least some progress it was decided to make other modifications were made in the meantime.

The code was modified to remove the repeated calculation, and this is shown in Figure~\ref{figure:GmmMod}
Two timing runs were performed resulting in times of 63.584596 and 63.588328 seconds for 100 steps for an average of 0.636 seconds per step.
This represents a rather small improvement of 2\% over the previous run time and 67\% over the original run time.
From this we conclude that while the extra calculations were making at least some difference, they were not a major limiting factor in the code's performance.

\begin{figure}
	\begin{lstlisting}
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] -
				  force(G * mass[i] * mass[j], delta_pos[l][k], delta_r[k]);
			f[l][j] = f[l][j] +
				  force(G * mass[i] * mass[j], delta_pos[l][k], delta_r[k]);
		}
	\end{lstlisting}
	\caption{Modified code with slowest index in outermost loop.}
	\label{figure:GmmOrig}
\end{figure}

\begin{figure}
	\begin{lstlisting}
		double Gmm = G * mass[i] * mass[j];
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] - force(Gmm, delta_pos[l][k], delta_r[k]);
			f[l][j] = f[l][j] + force(Gmm, delta_pos[l][k], delta_r[k]);
		}
	\end{lstlisting}
	\caption{Modified code without repeated calculations.}
	\label{figure:GmmMod}
\end{figure}

\subsection{Removal of Repeated Function Calls}
The next modification removed the repeated calls to function \textbf{force()} in the same loop as above.
Figure~\ref{figure:ForceMod} shows the replacement of two calls to this function with a single call which stores the result.

\begin{figure}
	\begin{lstlisting}
		double Gmm = G * mass[i] * mass[j];
		double gforce = force(Gmm, delta_pos[l][k], delta_r[k]);
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] - gforce;
			f[l][j] = f[l][j] + gforce;
		}
	\end{lstlisting}
	\caption{Modified code with only a single call to \textbf{force()}.}
	\label{figure:ForceMod}
\end{figure}

Two timing runs produced times of 60.736962 and 60.985466 seconds for an average of 0.609 seconds per step.
This represents a reduction in run time of 4\% over the previous modification and 69\% overall.
Again, the small difference suggests this was not a major limiting factor in the code's performance.

\subsection{Loop Swap in Calculation of Central Force}
The next modification was a simple loop swap in the calculation of the force between each particle and the central mass.
The outermost loop contained the fastest-moving index, {\em i}, and this was moved to the innermost loop as shown in Figures \ref{figure:CfOrig} and \ref{figure:CfMod}

\begin{figure}
	\begin{lstlisting}
		/* calculate central force */
		for (i = 0; i < Nbody; i++)
		{
			for (l = 0; l < Ndim; l++)
			{
				f[l][i] = f[l][i] - force(G * mass[i] * M_central, pos[l][i], r[i]);
			}
		}
	\end{lstlisting}
	\caption{Original central force calculation.}
	\label{figure:CfOrig}
\end{figure}

\begin{figure}
	\begin{lstlisting}
		/* calculate central force */
		for (l = 0; l < Ndim; l++)
		{
			for (i = 0; i < Nbody; i++)
			{
				f[l][i] = f[l][i] - force(G * mass[i] * M_central, pos[l][i], r[i]);
			}
		}
	\end{lstlisting}
	\caption{Central force calculation with loops swapped.}
	\label{figure:CfMod}
\end{figure}

Two timing runs produced times of 60.619624 and 60.683955 seconds for 100 steps, representing an average of .607 seconds per step.
This was an reduction in run time of 0.3\% over the previous modification and a 69\% improvement overall.
It was concluded that while the loop swap probably improved the efficiency of that part of the program, other factors elsewhere in the code were still dominant.

The inner loop in this code also contains a repeated calculation of a loop invariant: \textbf{G * M\_central}.
This could have been replaced with a constant calculated outside the loop, however there was insufficient time after other modifications had been implemented.

\subsection{Loop Swap in Calculation of Positions}

\subsection{Loop Swap in Calculation of Velocities}
\subsection{Reduced Function Calls}
\subsection{Array Padding}
\subsection{Loop Swap in Calculation of Pairwise Forces}
\subsection{Incrementing Pointers}
\subsection{Incrementing Pointers 1}

\section{Future Work}

Maintainability.

Readability.

Performance.

Data structures likely to be major culprit.
Data structures - all items for one particle instead of all particles for one item - to improve data locality.  However, two simple arrays being accessed sequentially in different cache lines could be fine.
Experiment with array sizes - is performance affected by powers of 2 in array sizes?  This would reveal caching problems.  
Further work: try compiler flags again.
Further work: loop tiling in the force calculation as mass[i] and mass[j] accessed together.


\section{Conclusion}
This section is the conclusion.

\begin{thebibliography}{100}

\bibitem{ref:PgiCC} {\em 2015 PGI Compiler User Guide} NVIDIA Corporation. pp 24-26.

\end{thebibliography}

\end{document}