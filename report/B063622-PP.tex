%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MSc HPC
% Performance Programming
% Coursework
% Exam no. B063622
%
% Report.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages.
%
\documentclass[11pt, oneside]{article}   % use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                    % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}            % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                    % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                         % TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{listings}


% load .eps files created by GnuPlot for epstopdf to convert to .pdf
\DeclareGraphicsExtensions{.eps}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Components.
\title{Performance Programming \\ Coursework}
\author{B063622}
\date{\today}


%
% The following defines an environment for including source with syntax hilighting.
% (Copied from stackoverflow.com/questions/3175105/how-to-insert-code-%into-a-latex-doc) 
% Could be useful if we want to include source code in the report. 
%
% To use a different language, overwrite the language paramter in the code. I.e. write: 
%	\lstset{language=bash}
% before the beginning of the listing. 

% Then enter your code: 
%	\begin{lstlisting}
%		source code here ...
%	\end{lstlisting}
%

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
language=c++,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{gray},
keywordstyle=\color{blue},
commentstyle=\color{dkgreen},
stringstyle=\color{mauve},
breaklines=false,
breakatwhitespace=true,   
tabsize=3
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The document itself.
%
\begin{document}

\pagenumbering{gobble}   % No page number on title page.
\maketitle

\newpage

\pagenumbering{roman}   % Preamble pages numbered in Roman numerals.
\tableofcontents

\newpage

\pagenumbering{arabic}  % Body of report numbered in Arabic.

\section{Introduction}
The following report describes the process of optimising a program which runs a molecular dynamics calculation.
The calculation models a number of particles which are subject to two forces: an inverse square attraction and a viscosity in the system.

A series of  modifications were made to the code in an attempt to optimise its performance, including several which degraded performance and which were discarded from the final code.
The modifications these are described, and an analysis of the effect of each modification on the code's performance is presented.

The code was provided in both C and Fortran; the C version was chosen for this exercise.

\section{First Steps}

\subsection{Examining the Code}
\label{subsec:ExaminingTheCode}

The first step was to study the code and its build process in order to gain an understanding of how the code worked and to identify potential modifications which might improve performance.
Several such possibilities were initially identified in the code and its make file:

\begin{itemize}
	\item Debug symbols enabled during compilation.
	\item Lack of compiler optimisation.
	\item Nested for loops with slowest-changing indices innermost.
	\item Loops which could be combined.
	\item Repeated calculations within loops.
	\item Loop-invariant values recalculated within loops.
	\item Branches within loops.
	\item Arrays sized as powers of 2.
	\item Related data in separate structures in memory.
	\item Multidimensional array indexing which could be converted to incrementing pointers.
\end{itemize}


\subsection{Recording Initial Performance}
An initial timing run was performed with the code and makefile unmodified.
The only change made to the provided files was to modify the Sun Grid Engine batch script \textbf{bench\_c.sge} to allocate 64 cores for each run as shown in Figure~\ref{figure:batchsge}.
This was done in order to isolate timing runs from the effects of any other programs running on the same node on MORAR.
This setting was applied to all timing runs performed.

\begin{figure}
	\begin{lstlisting}
		#$ -l h\_rt=:20:
		#$ -pe mpi 64
		#$ -cwd
	\end{lstlisting}
	\caption{Modification to bench\_c.sge to allocate 64 cores for timing runs.}
	\label{figure:batchsge}
\end{figure}

The program performed five iterations of 100 steps with each step including the running count of collisions and with each set of 100 steps indicating the time taken for that set.
Times for the initial run are shown in Table~\ref{table:InitialRunTimes}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{||l|c|l||}
			\hline
				{\bf Steps} & {\bf Time}\\
			\hline
				100  &  193.724736\\
				200  &  193.554321\\
				300  &  193.863500\\
				400  &  193.568124\\
				500  &  193.937730\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Initial run times per 100 steps.}
	\label{table:InitialRunTimes}
\end{table}

The initial timing was very consistent, resulting in an average of 1.937 seconds per step, with times for each set of 100 steps varying by less than one percent.
Based on these timings, it was decided that only the first four figures were significant as the fourth digit is the first to show (small) variation between measurements.
For this reason, two runs of 100 steps each were considered sufficient for timing the first few modifications to the code.
It was anticipated that times might become less consistent when the program became more efficient as interruptions for I/O would affect the program more if it were keeping the CPU busy.
However, run times remained consistent to within approximately three percent throughout the work done, so the number of timing runs for each modification was not increased.

\subsection{Testing for Correctness}

The program printed the number of the current step and the running count of collisions to the standard output along with the time taken for each set of 100 steps.
The program also wrote the positions and velocities of each particle in the system to an output file after each set.
In order to consider the program to still be correct after a modification, both the particle data and the collision count had to be sufficiently accurate.  

Perfect accuracy was not required as the order of floating point arithmetic in the code might be altered.
A test tool was provided which compared the values in two output files and reported the maximum difference between corresponding values as well as any differences which were greater than a chosen minimum and which were therefore unacceptable.  

The tool had a bug: it failed to handle NaN values correctly.
The code which performed the comparison between values reported a difference of 0.0 between any floating point number and NaN.
A check for NaN values was added and the code was made to indicate that the difference between any valid value and NaN, or between two NaNs, was the largest available double-precision floating point value.
The modified code is shown in Figure~\ref{figure:NanTest}.

\begin{figure}
	\begin{lstlisting}
		double error(double v1, double v2)
		{
			//  check for invalid values and return a 
			//  value greater than any reasonable maximum delta
			if (isnan(v1) || isnan(v2))
			{
				return DBL_MAX;
			}

			double diff = fabs(v1 - v2);
			double sum = fabs(v1 + v2);
			if (sum != 0.0)
			{
				// normalise
				diff = diff / sum;
			}
			return diff;
		}
	\end{lstlisting}
	\caption{Checking for NaN values in diff tool.}
	\label{figure:NanTest}
\end{figure}

\subsection{Input Data for Testing}
In order to produce output which could be tested for accuracy by comparing it with output from previous runs, the set of particle starting positions and velocities provided as input to the program had to be the same for each test run.
Additionally the program had to be run until collisions started to occur in order to verify that the collision count was correct.
A run of 100 steps was considered sufficient for both timing and accuracy as molecular dynamics calculations are generally very sensitive to small perturbations after many iterations.  

To ensure that collision counts were correct, the input used was a set of particle positions and velocities produced by the original program at a number of steps after collisions had begun occurring.
It was found that collisions began at 162 steps, so the positions and velocities at 200 steps, written to file output.dat200 by the first run of the program, were used as test run inputs.
The terst run's output file output.dat100 was then compared with the file output.dat100 produced by a run of the original unmodified code with the same input.
The program's output to stdout was compared manually to verify that the number of collisions had not changed.

A small difference in the number of collisions would have been acceptable in the same way as small differences in particle positions and velocities.
However, the number of collisions at 100 steps when starting with the selected input data was always 34107; this did not change throughout the exercise.

\subsection{Work Process}
Each modification to the program was performed in the following steps:

\begin{enumerate}
	\item The code or makefile was modified and the application built.
	\item The program was run twice on MORAR with output.dat200 specified as its input file.
	\item The test program was used to verify the accuracy of the ouput.
	\item The number of collisions after 100 steps was checked for any difference with the original value.
	\item The time for 100 steps was checked for any performance improvement.
	\item If performance was significantly degraded by the modification, it was reverted. 
\end{enumerate}

Modifications to the code were kept small and done one at a time in order to isolate the effects of any change.
Exceptions to this occurred when one modification enabled another,  for example where a function was called within a loop and inlining the function enabled the loop to be fused with other loops.

The accuracy of the final positions and velocities was verified using the test program.
The run time for 100 steps and the accuracy of the collision count were checked manually as this did not consume significant time.

\subsection{Profiling}

A profiling run was performed in order to gather information about the program's use of CPU time.
The profiling build was created by cleaning the previous build and passing the necessary profiling flags to the compiler on the command line as shown in Figure~\ref{figure:ProfMake}.

\begin{figure}
	\begin{lstlisting}
		make clean
		make CFLAGS="-g -pg"
	\end{lstlisting}
	\caption{Buildng with profiling enabled.}
	\label{figure:ProfMake}
\end{figure}

Output from the profiler is shown in Figure~\ref{figure:ProfOutput}.
\begin{figure}
	\begin{lstlisting}
		Each sample counts as 0.01 seconds.
		%   cumulative   self              self     total           
		time   seconds   seconds    calls   s/call   s/call  name    
		32.64    118.78   118.78                             _mcount2
		30.92    231.31   112.53 5033164800     0.00     0.00  force
		28.79    336.08   104.78        1   104.78   233.59  evolve
		4.46    352.31    16.23      600     0.03     0.03  add_norm
		2.48    361.33     9.02                             __forceEND
		0.52    363.23     1.89                             _mp_preinit
		0.17    363.83     0.60                             __rouinit
		0.01    363.86     0.03      300     0.00     0.00  visc_force
		0.01    363.88     0.02      300     0.00     0.00  wind_force
		0.01    363.90     0.02                             __rouexit
		0.00    363.90     0.00        2     0.00     0.00  second
	\end{lstlisting}
	\caption{Output from the profiler.}
	\label{figure:ProfOutput}
\end{figure}

It can be seen that most of the program's time is spent in functions \textbf{force()} and \textbf{evolve()}.
This result is unsurprising as function \textbf{evolve()} contains all of the program's calculation loops, and \textbf{force()} is called repeatedly in the innermost of a set of three nested for loops which together iterate over the entire range of four of the program's large arrays.

As function \textbf{evolve()} contained several for loops, the option of placing each of these in a separate function in order to use the profiler to determine which were taking the most time was considered.
However, there were already several clear possibilities for modifications (see Section~\ref{subsec:ExaminingTheCode}) which were expected to improve performance significantly, so this was reserved as an option for later if needed.

\section{Modifications to the Code}

\subsection{Removing Debug Symbols}
A quick and obvious modification to the program's makefile was to remove the \textbf{-g} flag from the CFLAGS variable as this adds debug symbols to the executable, increasing its size and degrading its performance.
Two timing runs were performed with the flag removed, and the resulting times for 100 steps were 107.066320 and 107.309783 seconds.
Taking the average of these times and comparing with the original value of 1.937 seconds per step, the reduction in run time was approximately 45\%.
This was unsurprising as enabling debug information when compiling typically reduces performance significantly.

\subsection{Compiler Optimisation Flags}
A second quick modification was to apply compiler optimisation flags in the program's makefile.
Flags \textbf{-O1} to \textbf{-O4} were tried as well as \textbf{-fast -Mipa=fast,inline}  as recommended by th PGI Compiler User Guide. \cite{ref:PgiCC}
Results are shown in Table~\ref{table:OptFlags}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{||l|c|l||}
			\hline
			{\bf Flags} & {\bf Time 1} & {\bf Time 2}\\
			\hline
				O1  &  125.075299  &  (n/a)\\
				O2  &  127.319100  &  (n/a)\\ 
				O3  &  127.375565  &  127.023777\\
				O4  &  125.678674  &  125.906789\\
				-fast -Mipa=fast,inline  &  95.240342  &  95.159813\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Run times with various compiler optimisations enabled.}
	\label{table:OptFlags}
\end{table}

A single run was done for each of \textbf{-O1} and \textbf{-O2} as these were not expected to significantly improve the program's 'run time.  
In fact both were found to increase run time.
Two runs were done for each of \textbf{-O3} and \textbf{-O4}, and these also increased run time.
Two runs were done with \textbf{-fast âMipa=fast,inline} and this was found to reduce run time by 11\% from the previous run time, representing a reduction of 51\% in run time from the original build of the program.

The compiler flags were therefore left as \textbf{-fast -Mipa=fast,inline} for subsequent modifications.

\subsection{Loop Swap in Calculation of Pairwise Separation}
The next modification was a loop swap in the program's calculation of distances between particles.
In the original code shown in Figure~\ref{figure:PairSepOrig} this was done with a set of three nested \textbf{for} loops with the slowest-moving index {\em l} in the innermost loop.

\begin{figure}
	\begin{lstlisting}
		/* calculate pairwise separation of particles */
		k = 0;
		for (i = 0; i < Nbody; i++)
		{
			for (j = i + 1; j < Nbody; j++)
			{
				for (l = 0; l < Ndim; l++)
				{
					delta_pos[l][k] = pos[l][i] - pos[l][j];
				}
				k = k + 1;
			}
		}
	\end{lstlisting}
	\caption{Original code with slowest index in innermost loop.}
	\label{figure:PairSepOrig}
\end{figure}

The innermost loop was moved to the outermost position and the initialisation and incrementing of variable {\em k} were moved to new positions.
The modified code is shown in Figure~\ref{figure:PairSepMod}.

\begin{figure}
	\begin{lstlisting}
		/* calculate pairwise separation of particles */
		for (l = 0; l < Ndim; l++)
		{
			k = 0;
			for (i = 0; i < Nbody; i++)
			{
				for (j = i + 1; j < Nbody; j++)
				{
					delta_pos[l][k] = pos[l][i] - pos[l][j];
					k = k + 1;
				}
			}
		}
	\end{lstlisting}
	\caption{Modified code with slowest index in outermost loop.}
	\label{figure:PairSepMod}
\end{figure}

Two timing runs of 100 steps produced times of 65.047681 and 65.043864 seconds, representing an average time per step of 0.650 seconds.
This represents a reduction in run time of 32\% over the previous code and a reduction of 66\% from the original program.

\subsection{Removal of Repeated Calculations}
The next modification was to remove two repeated calculations of the product of the gravitational constant and the masses of two particles in the calculation of the forces between pairs of particles.
The original code shown in Figure~\ref{figure:GmmOrig} calculates \textbf{G * mass[i] * mass[j]} twice unnecessarily.

This was taking place inside three nested for loops with the slowest-moving index in the innermost loop, and it was recognised that it would be beneficial to swap these loops in order to improve performance.
However, the original code was complicated enough that doing this took several attempts, and in order to make at least some progress it was decided to make other modifications were made in the meantime.

The code was modified to remove the repeated calculation, and this is shown in Figure~\ref{figure:GmmMod}
Two timing runs were performed resulting in times of 63.584596 and 63.588328 seconds for 100 steps for an average of 0.636 seconds per step.
This represents a rather small improvement of 2\% over the previous run time and 67\% over the original run time.
From this we conclude that while the extra calculations were making at least some difference, they were not a major limiting factor in the code's performance.

\begin{figure}
	\begin{lstlisting}
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] -
				  force(G * mass[i] * mass[j], delta_pos[l][k], delta_r[k]);
			f[l][j] = f[l][j] +
				  force(G * mass[i] * mass[j], delta_pos[l][k], delta_r[k]);
		}
	\end{lstlisting}
	\caption{Modified code with slowest index in outermost loop.}
	\label{figure:GmmOrig}
\end{figure}

\begin{figure}
	\begin{lstlisting}
		double Gmm = G * mass[i] * mass[j];
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] - force(Gmm, delta_pos[l][k], delta_r[k]);
			f[l][j] = f[l][j] + force(Gmm, delta_pos[l][k], delta_r[k]);
		}
	\end{lstlisting}
	\caption{Modified code without repeated calculations.}
	\label{figure:GmmMod}
\end{figure}

\subsection{Removal of Repeated Function Calls}
The next modification removed the repeated calls to function \textbf{force()} in the same loop as above.
Figure~\ref{figure:ForceMod} shows the replacement of two calls to this function with a single call which stores the result.

\begin{figure}
	\begin{lstlisting}
		double Gmm = G * mass[i] * mass[j];
		double gforce = force(Gmm, delta_pos[l][k], delta_r[k]);
		/*  flip force if close in */
		if (delta_r[k] >= Size)
		{
			f[l][i] = f[l][i] - gforce;
			f[l][j] = f[l][j] + gforce;
		}
	\end{lstlisting}
	\caption{Modified code with only a single call to \textbf{force()}.}
	\label{figure:ForceMod}
\end{figure}

Two timing runs produced times of 60.736962 and 60.985466 seconds for an average of 0.609 seconds per step.
This represents a reduction in run time of 4\% over the previous modification and 69\% overall.
Again, the small difference suggests the extra function call  was not a major limiting factor in the code's performance.

\subsection{Loop Swap in Calculation of Central Force}
The next modification was a simple loop swap in the calculation of the force between each particle and the central mass.
The outermost loop contained the fastest-moving index, {\em i}, and this was moved to the innermost loop as shown in Figures \ref{figure:CfOrig} and \ref{figure:CfMod}

\begin{figure}
	\begin{lstlisting}
		/* calculate central force */
		for (i = 0; i < Nbody; i++)
		{
			for (l = 0; l < Ndim; l++)
			{
				f[l][i] = f[l][i] - force(G * mass[i] * M_central, pos[l][i], r[i]);
			}
		}
	\end{lstlisting}
	\caption{Original central force calculation.}
	\label{figure:CfOrig}
\end{figure}

\begin{figure}
	\begin{lstlisting}
		/* calculate central force */
		for (l = 0; l < Ndim; l++)
		{
			for (i = 0; i < Nbody; i++)
			{
				f[l][i] = f[l][i] - force(G * mass[i] * M_central, pos[l][i], r[i]);
			}
		}
	\end{lstlisting}
	\caption{Central force calculation with loops swapped.}
	\label{figure:CfMod}
\end{figure}

Two timing runs produced times of 60.619624 and 60.683955 seconds for 100 steps, representing an average of .607 seconds per step.
This was an reduction in run time of 0.3\% over the previous modification and a 69\% improvement overall.
It was concluded that while the loop swap probably improved the efficiency of that part of the program, other factors elsewhere in the code were still dominant.

The inner loop in this code also contains a repeated calculation of a loop invariant: \textbf{G * M\_central}.
This could have been replaced with a constant calculated outside the loop, however there was insufficient time after other modifications had been implemented.

\subsection{Loop Swaps in Calculations of Positions and Velocities}
In the same manner as the previous modification, two loop swaps were performed on the code which calculated the new positions and velocities of the particles.
As in the previous modifications, the inner loops were iterating over the slowest-moving indices, and both loops were swapped so that the fastest-moving indices were used in the inner loops. 

Swapping loops in the position calculation produced run times of 59.966728 and 60.041090 seconds for an average of 0.600 seconds per step.
This represented a 1\% reduction over the previous run time, with overall reduction still at 69\%.

Swapping loops in the velocity calculation produced run times of 59.894412 and 59.977946 seconds for an average of 0.599 seconds per step.
This represented a reduction of less than 1\% over the previous run time, with overall reduction still at 69\%.

From this it is clear that these loop swaps were not a significant limiting factor in the code's performance.

\subsection{Reduced Function Calls}
The next modification was to inline a call to function \textbf{force()} in the addition of forces between pairs.
As this function was called once for every particle in the system, it was expected that removing this function call overhead would improve the performance of the loop.
The modification is shown in Figure~\ref{figure:CallMod} with the earlier code commented out.

\begin{figure}
	\begin{lstlisting}
		// before: function call in inner loop
		// double Gmm = G * mass[i] * mass[j];
		// double gforce = force(Gmm, delta_pos[l][k], delta_r[k]);

		// after: function call inlined
		double gforce = G * mass[i] * mass[j] * delta_pos[l][k] / pow(delta_r[k], 3.0);
	\end{lstlisting}
	\caption{Pairwise force calculation with function inlined.}
	\label{figure:CallMod}
\end{figure}

However, this modification made performance worse; run times were 62.618063 and 67.672446 seconds, for an average of 0.651 seconds per step.
It is not understood why the run times varied more than with other modifications.
However, it is clear that performance was degraded, perhaps due to register pressure caused by having more expressions in a C statement.

As this modification reduced performance, it was discarded.

\subsection{Array Padding}
The next modification was to pad the large data arrays used by the function.
All of these arrays were multiples of large powers of 2, meaning their positions in memory were likely to be aligned on the same cache lines and that cache conflicts were likely to be a serious factor in the program's' performance.
It was expected that padding the arrays by the size of a cache line (64 bytes on MORAR's AMD Opteron CPUs) in order to shift each consecutive array by one more cache line would enable an element at a given index in any one array to be accessed without causing the element at the same index in any other array to be flushed.
A pad value was calculated in terms of the size of a double floating point number (as used in the arrays) and this was added to the size of the memory allocation for each array as shown in Figure~\ref{figure:Pad}.

\begin{figure}
	\begin{lstlisting}
		/* calculate size in doubles of 1 line of AMD Opteron L3 cache */
		int const cache_line = 64;
		int const pad_doubles = 2 * cache_line / sizeof(double);

		r = calloc(Nbody + pad_doubles, sizeof(double));
		delta_r = calloc(Nbody * Nbody + pad_doubles, sizeof(double));

		// ...
	\end{lstlisting}
	\caption{Array allocations including pads.}
	\label{figure:Pad}
\end{figure}

Two timing runs of 100 steps each produced times of 63.811710 and 65.008981 seconds, for an average of 0.644 seconds per step.
This represented an increase in the program's run time.
It is not clear why this occurred; little or no change would have suggested that cache conflicts were not significantly affecting performance, but the reduction in performance is not well understood.

As this modification reduced performance, it was discarded.

\subsection{Loop Swap in Calculation of Pairwise Forces}
The next modification was to swap loops in the calculation of forces between pairs of particles. 
This had been identified early on as a highly probable source of cache misses and performance degradation, but the code was difficult to modify as it was optimised to reduce memory usage in the storage of pair separations in {\em delta\_r}.
This was an example of how optimisation can reduce code maintainability; the modification was attempted several times before a correct solution was found.

The resulting code is shown in Figure~\ref{figure:PairSwap}.

\begin{figure}
	\begin{lstlisting}
		// add pairwise forces.
		int local_collisions = 0;
		for (l = 0; l < Ndim; l++)
		{
			k = 0;
			for (i = 0; i < Nbody; i++)
			{
				for (j = i + 1; j < Nbody; j++)
				{
					/*  
					* flip force if close in - without branching within the inner loop 
					*/
					double multiplier = 1.0;
					if (! (delta_r[k] >= Size))
					{
						multiplier = -1.0;
						local_collisions++;
					}

					double gforce = multiplier * G * mass[i] * mass[j] * delta_pos[l][k] / pow(delta_r[k], 3.0);

					f[l][i] = f[l][i] - gforce;
					f[l][j] = f[l][j] + gforce;

					k = k + 1;
				}
			}
		}
		// this is a kludge: the modified code results in 
		// 3x as many collisions so we correct this here
		collisions += local_collisions / 3;
	\end{lstlisting}
	\caption{Pairwise separation calculation with loops swapped.}
	\label{figure:PairSwap}
\end{figure}

Two timing runs produced times of 50.109335 and 50.124341 seconds for 100 steps, for an average time of 0.501 seconds per step.
This was a 22\% reduction in run time, suggesting that cache misses in the innermost of the three loops had been a significant factor due to stepping through the arrays by large increments.
Overall reduction in program time was now 74\%.

\subsection{Incrementing Pointers}
The next modification was to replace array accesses via indexing with access via incrementing pointers within the loops which calculated new particle positions and velocities in order to reduce arithmetic when accessing array elements.
The resulting code for the position calculation is shown in Figure~\ref{figure:PInc}.
A corresponding change was made to the velocity calculation.

\begin{figure}
	\begin{lstlisting}
		double *pvel = vel[0];
		double *pvel = vel[0]
		for (j = 0; j < Ndim; j++)
		{
			for (i = 0; i < Nbody; i++)
			{
				// pos[j][i] = pos[j][i] + dt * vel[j][i];
				*ppos += dt * *pvel;
				++ppos;
				++pvel;
			}
		}
	\end{lstlisting}
	\caption{Position update with incrementing pointers.}
	\label{figure:PInc}
\end{figure}

The resulting code is less readable, as is often the case with optimisation.
In this case there were additional variables required in order to store the current position in each array as well as more code to update the pointers.
Modifying this code in future will therefore require more time to understand the code and more lines of code to be changed, with more opportunities for error or omission.

Two runs with this modification produced times of 50.766400 and 51.394138 seconds for 100 steps, representing an average of 0.511 seconds per step.
This was a small reduction in performance, which suggests that the compiler was already performing this optimisation and that the manual implementation was either less efficient or interfering with the compiler's efforts.

As this modification reduced performance, it was discarded.

\section{Future Work}

There are several further modifications which may produce significant improvements in the program's performance.
Several of these were identified during the initial examination of the code in Section~\ref{subsec:ExaminingTheCode} including more loops which could be swapped or combined, repeated calculations which could be removed, and storing related data in separate arrays in memory. 

The most likely candidate for significant improvement is the arrangement of data in memory.
Each calculation requiring elements of several different arrays might perform better if all of the elements with a given index were stored in a data structure and the entirety of the data stored in an array of these structures.
This would improve data locality.
However, this modification would be quite time-consuming as the arrays and all code which references them would have to be reworked in order to use the new data structure.

It is also possible that loop tiling in the code which calculated the forces between pairs may improve performance as the {\em i} and {\em j} indices into the {\em mass} array started at similar values and moved to the value of {\em NBody}.  

It may be of value to test the program with various numbers of particles in order to see if cache conflicts are being caused by the number of particles being a power of 2.

Finally, several sections of the code appear to be highly parallelisable; they iterate through arrays and update their elements with calculations which are independent of any other elements.
However, the scope of this exercise did not include parallelising the code.

\section{Conclusion}

The initial examination of the code identified many potential improvements.
There was insufficient time for all of these, so those which were easiest to attempt were done first, bearing in mind that not all modifications were guaranteed to actually produce performance improvements.
This was found to be the case as several modifications which were expected to improve performance actually worsened it, as in the case of replacing array indexing with incrementing pointers.

The program's run time per step was reduced from 1.937 seconds to 0.501, representing a reduction of 74\%.
If we define program speedup as 

\begin{equation}
S = T_{ref}/T
\label{equation:speedup}
\end{equation}

...where ${T}$ is the program's current run time and $T_{ref}$ is the run time of the original code, we find that the final program speedup was 3.87.
The program's performance could likely be improved further by a complete reworking of the code, rearranging the data structures to keep elements which were needed together in calculations close to each other in memory, and updating all of the calculation code to match the new data structures.
This would however represent a much larger investment of time than the modifications which were attempted.


\begin{thebibliography}{100}

\bibitem{ref:PgiCC} {\em 2015 PGI Compiler User Guide} NVIDIA Corporation. pp 24-26.

\end{thebibliography}

\end{document}